# KMU_application
# Model Stability and Performance Evaluation in Response to Prompt Variability

## Overview

In recent years, prompting as a directed interaction methodology with machine learning models has gained increasing popularity. This method provides the ability to control the output of models by providing them with specific textual input data or prompts. In this work, our main goal is to improve the assessment of language models, based on the assumption that a good model should be stable to the variability of prompts. 

To achieve this, we proposed a methodology to test the hypothesis of the stability coefficient of language models to prompt changes and evaluated its potential implications for practical use and understanding of the issue. We conducted a series of experiments on tasks from the MERA benchmark, focusing on how the models respond to different prompt variations.

## Methodology

### Task Selection and Dataset

We selected tasks from the MERA benchmark, specifically focusing on four tasks: Chegeka, LCS, ruDetox, and ruOpenBookQA. For each task, we sampled datasets to gather approximately 10 examples for initial hypothesis testing. We then generated 10 variations of task texts for each selected task, ranging from brief to extensive.

### Models and Generation Settings

To evaluate model performance, we used the Vikhr, TinyLlama, and other models, setting the temperature to 0 to eliminate random responses and focus on their generation results on different prompts. 

### Evaluation Metrics

In the context of evaluating language models and their response to rapid changes, it is important to establish reliable evaluation metrics that accurately reflect the performance and stability of the models. Traditional evaluation metrics in natural language processing tasks, such as BLEU scores and classification metrics, provide valuable information about the model's performance but may not fully reflect nuances related to prompt variations. 

One of the key issues to address is considering the stability and reliability of models in the face of changing input data or prompts. Currently, there is no standard approach to account for model stability in response to changes in input prompts, limiting our ability to meaningfully evaluate and compare models.

### Stability Coefficient

The stability coefficient can be calculated by measuring the similarity or consistency of model results when presented with different prompts for the same task. This involves calculating cosine similarity or other similarity metrics between the output data generated by the model in response to different prompts. A higher stability coefficient indicates that the model provides more consistent responses regardless of prompt changes, reflecting its stability in processing various input conditions.

### Visualization and Interpretation

During the analysis, we applied interpretation methods, including SHAP, to visualize which parts of the input prompts the models paid attention to when forming responses. This approach allowed us to gain insights into how the model's attention and understanding could vary depending on prompt variations.

